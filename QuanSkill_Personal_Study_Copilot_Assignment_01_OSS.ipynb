{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0a83c02b",
      "metadata": {
        "id": "0a83c02b"
      },
      "source": [
        "# ğŸ§  QuanSkill's Personal Study Copilot â€“ Assignment 01 Notebook (Openâ€‘Source Version)\n",
        "# ğŸ§  QuanSkill's Personal Study Copilot â€“ Assignment 01 Notebook (PhiÃªn báº£n MÃ£ nguá»“n má»Ÿ)\n",
        "\n",
        "This is your **guided assignment notebook** for building a _Personal Study Copilot_ agent.\n",
        "ÄÃ¢y lÃ  **notebook hÆ°á»›ng dáº«n** Ä‘á»ƒ xÃ¢y dá»±ng má»™t agent _Personal Study Copilot_ (Trá»£ lÃ½ há»c táº­p cÃ¡ nhÃ¢n).\n",
        "\n",
        "In this version, we **do not use any paid API keys**. Everything runs on **openâ€‘source models**:\n",
        "Trong phiÃªn báº£n nÃ y, chÃºng ta **khÃ´ng sá»­ dá»¥ng báº¥t ká»³ API key tráº£ phÃ­ nÃ o**. Má»i thá»© cháº¡y trÃªn **cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ**:\n",
        "\n",
        "- Text embeddings from `sentence-transformers/all-MiniLM-L6-v2`\n",
        "- Embeddings vÄƒn báº£n tá»« `sentence-transformers/all-MiniLM-L6-v2`\n",
        "- A small openâ€‘source chat model via `transformers` + `HuggingFacePipeline` (e.g., TinyLlama)\n",
        "- MÃ´ hÃ¬nh chat mÃ£ nguá»“n má»Ÿ nhá» qua `transformers` + `HuggingFacePipeline` (vÃ­ dá»¥: TinyLlama)\n",
        "\n",
        "You will:\n",
        "Báº¡n sáº½:\n",
        "\n",
        "- Ingest study documents\n",
        "- Náº¡p cÃ¡c tÃ i liá»‡u há»c táº­p\n",
        "- Build a vector store (FAISS)\n",
        "- XÃ¢y dá»±ng kho lÆ°u trá»¯ vector (FAISS)\n",
        "- Create a RAG chain using LCEL\n",
        "- Táº¡o chuá»—i RAG sá»­ dá»¥ng LCEL\n",
        "- Add simple conversational memory\n",
        "- ThÃªm bá»™ nhá»› há»™i thoáº¡i Ä‘Æ¡n giáº£n\n",
        "- Build a tool (calculator or similar)\n",
        "- XÃ¢y dá»±ng má»™t cÃ´ng cá»¥ (mÃ¡y tÃ­nh hoáº·c tÆ°Æ¡ng tá»±)\n",
        "- Use LangGraph to route between RAG and the tool\n",
        "- Sá»­ dá»¥ng LangGraph Ä‘á»ƒ Ä‘á»‹nh tuyáº¿n giá»¯a RAG vÃ  cÃ´ng cá»¥\n",
        "- Expose a `chat_loop()` to talk to your agent\n",
        "- Táº¡o má»™t `chat_loop()` Ä‘á»ƒ trÃ² chuyá»‡n vá»›i agent cá»§a báº¡n\n",
        "\n",
        "> ğŸ”§ This notebook contains explanations + **TODOs**. Your job is to fill them in.\n",
        "> ğŸ”§ Notebook nÃ y chá»©a cÃ¡c giáº£i thÃ­ch + **TODOs**. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  Ä‘iá»n vÃ o chÃºng.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8977dfa6",
      "metadata": {
        "id": "8977dfa6"
      },
      "source": [
        "## âœ… High-Level Checklist\n",
        "## âœ… Danh sÃ¡ch kiá»ƒm tra tá»•ng quan\n",
        "\n",
        "By the end, your notebook should:\n",
        "Äáº¿n cuá»‘i notebook, báº¡n nÃªn cÃ³:\n",
        "\n",
        "1. Ingest `.txt` / `.md` documents into a vector store.\n",
        "1. Náº¡p cÃ¡c tÃ i liá»‡u `.txt` / `.md` vÃ o kho lÆ°u trá»¯ vector.\n",
        "2. Implement `retrieve_context(query: str) -> List[str]`.\n",
        "2. Triá»ƒn khai `retrieve_context(query: str) -> List[str]`.\n",
        "3. Implement a LCEL-style RAG chain + `answer_with_rag(query: str)`.\n",
        "3. Triá»ƒn khai chuá»—i RAG kiá»ƒu LCEL + `answer_with_rag(query: str)`.\n",
        "4. Maintain some conversation memory.\n",
        "4. Duy trÃ¬ bá»™ nhá»› há»™i thoáº¡i.\n",
        "5. Use LangGraph with:\n",
        "5. Sá»­ dá»¥ng LangGraph vá»›i:\n",
        "   - `router` node (decides RAG vs Tool)\n",
        "   - NÃºt `router` (quyáº¿t Ä‘á»‹nh RAG hay Tool)\n",
        "   - `rag_node`\n",
        "   - NÃºt `rag_node`\n",
        "   - `tool_node` (calculator or other)\n",
        "   - NÃºt `tool_node` (mÃ¡y tÃ­nh hoáº·c cÃ´ng cá»¥ khÃ¡c)\n",
        "6. Provide a `chat_loop()` to interact with the agent.\n",
        "6. Cung cáº¥p `chat_loop()` Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "962b6874",
      "metadata": {
        "id": "962b6874"
      },
      "source": [
        "## 0. Environment Setup (Openâ€‘Source Stack)\n",
        "## 0. Thiáº¿t láº­p mÃ´i trÆ°á»ng (NgÄƒn xáº¿p mÃ£ nguá»“n má»Ÿ)\n",
        "\n",
        "We will use these packages:\n",
        "ChÃºng ta sáº½ sá»­ dá»¥ng cÃ¡c gÃ³i sau:\n",
        "\n",
        "- `langchain`, `langchain-community` â€“ core + community integrations\n",
        "- `langchain`, `langchain-community` â€“ lÃµi + tÃ­ch há»£p cá»™ng Ä‘á»“ng\n",
        "- `langgraph` â€“ to define the agent graph\n",
        "- `langgraph` â€“ Ä‘á»ƒ Ä‘á»‹nh nghÄ©a Ä‘á»“ thá»‹ agent\n",
        "- `faiss-cpu` â€“ vector store backend\n",
        "- `faiss-cpu` â€“ backend kho lÆ°u trá»¯ vector\n",
        "- `transformers`, `accelerate` â€“ to run openâ€‘source LLMs\n",
        "- `transformers`, `accelerate` â€“ Ä‘á»ƒ cháº¡y cÃ¡c LLM mÃ£ nguá»“n má»Ÿ\n",
        "- `sentence-transformers` â€“ for embeddings\n",
        "- `sentence-transformers` â€“ cho embeddings\n",
        "\n",
        "Run the cell below once to install them if needed.\n",
        "Cháº¡y cell bÃªn dÆ°á»›i má»™t láº§n Ä‘á»ƒ cÃ i Ä‘áº·t chÃºng náº¿u cáº§n.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a672da",
      "metadata": {
        "id": "e1a672da"
      },
      "outputs": [],
      "source": [
        "# âš™ï¸ Install dependencies (uncomment if needed)\n",
        "# âš™ï¸ CÃ i Ä‘áº·t cÃ¡c phá»¥ thuá»™c (bá» comment náº¿u cáº§n)\n",
        "\n",
        "# !pip install -q langchain langchain-community langgraph\n",
        "# !pip install -q faiss-cpu\n",
        "# !pip install -q transformers accelerate sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701881a2",
      "metadata": {
        "id": "701881a2"
      },
      "source": [
        "## 1. Imports & Basic Configuration\n",
        "## 1. Import vÃ  Cáº¥u hÃ¬nh cÆ¡ báº£n\n",
        "\n",
        "Here we import everything we need for the assignment.\n",
        "á» Ä‘Ã¢y chÃºng ta import má»i thá»© cáº§n thiáº¿t cho bÃ i táº­p.\n",
        "You may remove imports later if you don't use them.\n",
        "Báº¡n cÃ³ thá»ƒ xÃ³a cÃ¡c import sau nÃ y náº¿u khÃ´ng sá»­ dá»¥ng chÃºng.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "594550ea",
      "metadata": {
        "id": "594550ea"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any\n",
        "from dataclasses import dataclass, field\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "\n",
        "# LangChain core\n",
        "# LÃµi LangChain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Vector store + embeddings (openâ€‘source)\n",
        "# Kho lÆ°u trá»¯ vector + embeddings (mÃ£ nguá»“n má»Ÿ)\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "# Text splitter\n",
        "# Bá»™ chia vÄƒn báº£n\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# LangGraph\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Transformers for openâ€‘source LLM\n",
        "# Transformers cho LLM mÃ£ nguá»“n má»Ÿ\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82deb008",
      "metadata": {
        "id": "82deb008"
      },
      "source": [
        "## 2. Agent State (LangGraph)\n",
        "## 2. Tráº¡ng thÃ¡i Agent (LangGraph)\n",
        "\n",
        "We define a simple state class to track what flows through the agent graph.\n",
        "ChÃºng ta Ä‘á»‹nh nghÄ©a má»™t lá»›p tráº¡ng thÃ¡i Ä‘Æ¡n giáº£n Ä‘á»ƒ theo dÃµi nhá»¯ng gÃ¬ cháº£y qua Ä‘á»“ thá»‹ agent.\n",
        "You can extend it if needed, but keep these fields at minimum.\n",
        "Báº¡n cÃ³ thá»ƒ má»Ÿ rá»™ng nÃ³ náº¿u cáº§n, nhÆ°ng giá»¯ tá»‘i thiá»ƒu cÃ¡c trÆ°á»ng nÃ y.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa1acdf6",
      "metadata": {
        "id": "fa1acdf6"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AgentState:\n",
        "    user_input: str  # Äáº§u vÃ o tá»« ngÆ°á»i dÃ¹ng\n",
        "    messages: List[Dict[str, Any]] = field(default_factory=list)  # Danh sÃ¡ch tin nháº¯n\n",
        "    route: str = ''          # 'rag' or 'tool' - 'rag' hoáº·c 'tool'\n",
        "    tool_result: Any = None  # output from tool - Káº¿t quáº£ tá»« cÃ´ng cá»¥\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8648ebbc",
      "metadata": {
        "id": "8648ebbc"
      },
      "source": [
        "## 3. Document Ingestion & Vector Store\n",
        "## 3. Náº¡p tÃ i liá»‡u vÃ  Kho lÆ°u trá»¯ Vector\n",
        "\n",
        "Steps:\n",
        "CÃ¡c bÆ°á»›c:\n",
        "\n",
        "1. Point `DOCS_FOLDER` to a directory with `.txt` / `.md` study files.\n",
        "1. Trá» `DOCS_FOLDER` Ä‘áº¿n thÆ° má»¥c chá»©a cÃ¡c file há»c táº­p `.txt` / `.md`.\n",
        "2. Load all files and read them as raw strings.\n",
        "2. Táº£i táº¥t cáº£ cÃ¡c file vÃ  Ä‘á»c chÃºng dÆ°á»›i dáº¡ng chuá»—i thÃ´.\n",
        "3. Split the raw texts into chunks using `RecursiveCharacterTextSplitter`.\n",
        "3. Chia cÃ¡c vÄƒn báº£n thÃ´ thÃ nh cÃ¡c Ä‘oáº¡n báº±ng `RecursiveCharacterTextSplitter`.\n",
        "4. Use `HuggingFaceEmbeddings` + `FAISS.from_documents` to build a vector store.\n",
        "4. Sá»­ dá»¥ng `HuggingFaceEmbeddings` + `FAISS.from_documents` Ä‘á»ƒ xÃ¢y dá»±ng kho lÆ°u trá»¯ vector.\n",
        "\n",
        "> âœ… At the end, you should have a global `vectorstore` object.\n",
        "> âœ… Cuá»‘i cÃ¹ng, báº¡n nÃªn cÃ³ má»™t Ä‘á»‘i tÆ°á»£ng `vectorstore` toÃ n cá»¥c.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdd02a4f",
      "metadata": {
        "id": "fdd02a4f"
      },
      "outputs": [],
      "source": [
        "# TODO: change this path if needed (e.g., '/content/docs' in Colab)\n",
        "# TODO: thay Ä‘á»•i Ä‘Æ°á»ng dáº«n nÃ y náº¿u cáº§n (vÃ­ dá»¥: '/content/docs' trong Colab)\n",
        "DOCS_FOLDER = 'data/docs'\n",
        "\n",
        "os.makedirs(DOCS_FOLDER, exist_ok=True)\n",
        "\n",
        "def load_raw_documents(docs_folder: str) -> List[str]:\n",
        "    # Load all .txt and .md files from docs_folder as a list of strings.\n",
        "    # Táº£i táº¥t cáº£ cÃ¡c file .txt vÃ  .md tá»« docs_folder dÆ°á»›i dáº¡ng danh sÃ¡ch chuá»—i.\n",
        "    texts = []\n",
        "    for fname in os.listdir(docs_folder):\n",
        "        fpath = os.path.join(docs_folder, fname)\n",
        "        if not os.path.isfile(fpath):\n",
        "            continue\n",
        "        if fname.lower().endswith(('.txt', '.md')):\n",
        "            with open(fpath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "raw_docs = load_raw_documents(DOCS_FOLDER)\n",
        "print(f'Loaded {len(raw_docs)} raw documents.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506ef7b0",
      "metadata": {
        "id": "506ef7b0"
      },
      "outputs": [],
      "source": [
        "# Split documents into chunks\n",
        "# Chia tÃ i liá»‡u thÃ nh cÃ¡c Ä‘oáº¡n\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # KÃ­ch thÆ°á»›c má»—i Ä‘oáº¡n: 1000 kÃ½ tá»±\n",
        "    chunk_overlap=200,    # Äá»™ chá»“ng láº¥p giá»¯a cÃ¡c Ä‘oáº¡n: 200 kÃ½ tá»±\n",
        "    add_start_index=True, # ThÃªm chá»‰ sá»‘ báº¯t Ä‘áº§u\n",
        ")\n",
        "\n",
        "all_splits = text_splitter.create_documents(raw_docs)\n",
        "print(f'Created {len(all_splits)} text chunks.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b3b0de",
      "metadata": {
        "id": "82b3b0de"
      },
      "outputs": [],
      "source": [
        "# TODO: Build vectorstore using openâ€‘source embeddings (HuggingFaceEmbeddings) + FAISS\n",
        "# TODO: XÃ¢y dá»±ng vectorstore sá»­ dá»¥ng embeddings mÃ£ nguá»“n má»Ÿ (HuggingFaceEmbeddings) + FAISS\n",
        "\n",
        "# Example (you may keep or modify):\n",
        "# VÃ­ dá»¥ (báº¡n cÃ³ thá»ƒ giá»¯ hoáº·c sá»­a Ä‘á»•i):\n",
        "# embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "# vectorstore = FAISS.from_documents(all_splits, embeddings)\n",
        "\n",
        "vectorstore = None  # ğŸ”´ TODO: replace with real vectorstore - ğŸ”´ TODO: thay tháº¿ báº±ng vectorstore tháº­t\n",
        "\n",
        "if vectorstore is None:\n",
        "    print('âš ï¸ vectorstore is still None â€“ implement it above.')\n",
        "    print('âš ï¸ vectorstore váº«n lÃ  None â€“ hÃ£y triá»ƒn khai nÃ³ á»Ÿ trÃªn.')\n",
        "else:\n",
        "    print('âœ… Vector store created.')\n",
        "    print('âœ… ÄÃ£ táº¡o kho lÆ°u trá»¯ vector.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d70424",
      "metadata": {
        "id": "c2d70424"
      },
      "source": [
        "## 4. Retriever â€“ `retrieve_context`\n",
        "## 4. Retriever â€“ `retrieve_context`\n",
        "\n",
        "Once `vectorstore` is ready, create a retriever and implement:\n",
        "Khi `vectorstore` Ä‘Ã£ sáºµn sÃ ng, táº¡o má»™t retriever vÃ  triá»ƒn khai:\n",
        "\n",
        "`retrieve_context(query: str, k: int = 4) -> List[str]`\n",
        "\n",
        "This should return a list of relevant text chunks.\n",
        "HÃ m nÃ y nÃªn tráº£ vá» danh sÃ¡ch cÃ¡c Ä‘oáº¡n vÄƒn báº£n liÃªn quan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dff922a7",
      "metadata": {
        "id": "dff922a7"
      },
      "outputs": [],
      "source": [
        "# TODO: create retriever from vectorstore\n",
        "# TODO: táº¡o retriever tá»« vectorstore\n",
        "retriever = None  # e.g., vectorstore.as_retriever(search_kwargs={'k': 4})\n",
        "                  # vÃ­ dá»¥: vectorstore.as_retriever(search_kwargs={'k': 4})\n",
        "\n",
        "def retrieve_context(query: str, k: int = 4) -> List[str]:\n",
        "    # Return top-k relevant text chunks for the query using retriever.\n",
        "    # Tráº£ vá» top-k Ä‘oáº¡n vÄƒn báº£n liÃªn quan cho truy váº¥n sá»­ dá»¥ng retriever.\n",
        "    if retriever is None:\n",
        "        raise ValueError('retriever is not initialized â€“ create it from vectorstore.')\n",
        "        raise ValueError('retriever chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o â€“ hÃ£y táº¡o nÃ³ tá»« vectorstore.')\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    return [d.page_content for d in docs]\n",
        "\n",
        "# You can test this later after building the vectorstore and retriever.\n",
        "# Báº¡n cÃ³ thá»ƒ kiá»ƒm tra Ä‘iá»u nÃ y sau khi xÃ¢y dá»±ng vectorstore vÃ  retriever.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5719c34e",
      "metadata": {
        "id": "5719c34e"
      },
      "source": [
        "## 5. Openâ€‘Source LLM + RAG Chain\n",
        "## 5. LLM mÃ£ nguá»“n má»Ÿ + Chuá»—i RAG\n",
        "\n",
        "Now we set up:\n",
        "BÃ¢y giá» chÃºng ta thiáº¿t láº­p:\n",
        "\n",
        "1. An openâ€‘source LLM via `transformers` + `HuggingFacePipeline`.\n",
        "1. Má»™t LLM mÃ£ nguá»“n má»Ÿ qua `transformers` + `HuggingFacePipeline`.\n",
        "2. A `ChatPromptTemplate` for RAG.\n",
        "2. Má»™t `ChatPromptTemplate` cho RAG.\n",
        "3. A LCEL chain: `lookup_context -> prompt -> llm`.\n",
        "3. Má»™t chuá»—i LCEL: `lookup_context -> prompt -> llm`.\n",
        "4. A helper function `answer_with_rag(query: str) -> str`.\n",
        "4. Má»™t hÃ m helper `answer_with_rag(query: str) -> str`.\n",
        "\n",
        "Suggested small chat model (no key needed):\n",
        "MÃ´ hÃ¬nh chat nhá» Ä‘Æ°á»£c Ä‘á» xuáº¥t (khÃ´ng cáº§n key):\n",
        "\n",
        "- `TinyLlama/TinyLlama-1.1B-Chat-v1.0`\n",
        "\n",
        "You may change to a different model, but keep it reasonably small.\n",
        "Báº¡n cÃ³ thá»ƒ Ä‘á»•i sang mÃ´ hÃ¬nh khÃ¡c, nhÆ°ng hÃ£y giá»¯ nÃ³ á»Ÿ kÃ­ch thÆ°á»›c há»£p lÃ½.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f7f4a0",
      "metadata": {
        "id": "37f7f4a0"
      },
      "outputs": [],
      "source": [
        "# TODO: Initialize openâ€‘source LLM using transformers + HuggingFacePipeline\n",
        "# TODO: Khá»Ÿi táº¡o LLM mÃ£ nguá»“n má»Ÿ sá»­ dá»¥ng transformers + HuggingFacePipeline\n",
        "\n",
        "# Example (you can adapt hyperparameters as needed):\n",
        "# VÃ­ dá»¥ (báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c siÃªu tham sá»‘ náº¿u cáº§n):\n",
        "# model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "# gen_pipe = pipeline('text-generation', model=model, tokenizer=tokenizer,\n",
        "#                     max_new_tokens=256, do_sample=False)\n",
        "# llm = HuggingFacePipeline(pipeline=gen_pipe)\n",
        "\n",
        "llm = None  # ğŸ”´ TODO: replace with real HuggingFacePipeline-based llm\n",
        "            # ğŸ”´ TODO: thay tháº¿ báº±ng llm dá»±a trÃªn HuggingFacePipeline tháº­t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d36b877",
      "metadata": {
        "id": "8d36b877"
      },
      "outputs": [],
      "source": [
        "# Prompt template for RAG\n",
        "# Máº«u prompt cho RAG\n",
        "rag_prompt = ChatPromptTemplate.from_template(\n",
        "    'You are a helpful study assistant.\\n\\n'\n",
        "    'Use ONLY the following context to answer the question.\\n'\n",
        "    'If the answer is not in the context, say: \"I do not see this in the study materials.\"\\n\\n'\n",
        "    'Context:\\n{context}\\n\\n'\n",
        "    'Question:\\n{question}\\n\\n'\n",
        "    'Give a clear and concise answer.'\n",
        ")\n",
        "\n",
        "def _lookup_context(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # HÃ m tra cá»©u ngá»¯ cáº£nh tá»« retriever\n",
        "    query = inputs['question']\n",
        "    chunks = retrieve_context(query)\n",
        "    return {\n",
        "        'context': '\\n\\n'.join(chunks),\n",
        "        'question': query,\n",
        "    }\n",
        "\n",
        "# TODO: Build LCEL RAG chain only after llm is initialized\n",
        "# TODO: XÃ¢y dá»±ng chuá»—i RAG LCEL chá»‰ sau khi llm Ä‘Æ°á»£c khá»Ÿi táº¡o\n",
        "lookup = RunnableLambda(_lookup_context)\n",
        "rag_chain = None  # ğŸ”´ TODO: set to lookup | rag_prompt | llm\n",
        "                  # ğŸ”´ TODO: Ä‘áº·t thÃ nh lookup | rag_prompt | llm\n",
        "\n",
        "def answer_with_rag(query: str) -> str:\n",
        "    # Use retrieval + llm to answer a question.\n",
        "    # Sá»­ dá»¥ng retrieval + llm Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.\n",
        "    if rag_chain is None:\n",
        "        raise ValueError('rag_chain is not initialized â€“ build it after llm is ready.')\n",
        "        raise ValueError('rag_chain chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o â€“ hÃ£y xÃ¢y dá»±ng nÃ³ sau khi llm sáºµn sÃ ng.')\n",
        "    result = rag_chain.invoke({'question': query})\n",
        "    # HuggingFacePipeline returns a string or generation object depending on config.\n",
        "    # HuggingFacePipeline tráº£ vá» má»™t chuá»—i hoáº·c Ä‘á»‘i tÆ°á»£ng generation tÃ¹y theo cáº¥u hÃ¬nh.\n",
        "    return str(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38140e3b",
      "metadata": {
        "id": "38140e3b"
      },
      "source": [
        "## 6. Conversational Memory (Simple)\n",
        "## 6. Bá»™ nhá»› há»™i thoáº¡i (ÄÆ¡n giáº£n)\n",
        "\n",
        "We use a simple Python list as memory:\n",
        "ChÃºng ta sá»­ dá»¥ng má»™t danh sÃ¡ch Python Ä‘Æ¡n giáº£n lÃ m bá»™ nhá»›:\n",
        "\n",
        "- `remember_message(role, content)`\n",
        "- `remember_message(role, content)` - Ghi nhá»› tin nháº¯n\n",
        "- `get_formatted_history()` (optional, if you want to inject history into prompts later)\n",
        "- `get_formatted_history()` (tÃ¹y chá»n, náº¿u báº¡n muá»‘n chÃ¨n lá»‹ch sá»­ vÃ o prompts sau nÃ y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c5d4605",
      "metadata": {
        "id": "5c5d4605"
      },
      "outputs": [],
      "source": [
        "# LÆ°u trá»¯ lá»‹ch sá»­ há»™i thoáº¡i\n",
        "conversation_history: List[Dict[str, str]] = []\n",
        "\n",
        "def remember_message(role: str, content: str):\n",
        "    # Ghi nhá»› má»™t tin nháº¯n vÃ o lá»‹ch sá»­ há»™i thoáº¡i\n",
        "    conversation_history.append({'role': role, 'content': content})\n",
        "\n",
        "def get_formatted_history() -> str:\n",
        "    # Láº¥y lá»‹ch sá»­ há»™i thoáº¡i Ä‘Ã£ Ä‘á»‹nh dáº¡ng\n",
        "    lines = []\n",
        "    for msg in conversation_history:\n",
        "        lines.append(f\"{msg['role'].upper()}: {msg['content']}\")\n",
        "    return '\\n'.join(lines)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28df1e9d",
      "metadata": {
        "id": "28df1e9d"
      },
      "source": [
        "## 7. Tool Implementation (Calculator Example)\n",
        "## 7. Triá»ƒn khai cÃ´ng cá»¥ (VÃ­ dá»¥ mÃ¡y tÃ­nh)\n",
        "\n",
        "Implement at least one tool. Here we suggest a **calculator**:\n",
        "Triá»ƒn khai Ã­t nháº¥t má»™t cÃ´ng cá»¥. á» Ä‘Ã¢y chÃºng ta Ä‘á» xuáº¥t má»™t **mÃ¡y tÃ­nh**:\n",
        "\n",
        "- Input: a math expression string (e.g., `2 + 3 * 4`).\n",
        "- Äáº§u vÃ o: má»™t chuá»—i biá»ƒu thá»©c toÃ¡n há»c (vÃ­ dá»¥: `2 + 3 * 4`).\n",
        "- Output: a safe evaluation result.\n",
        "- Äáº§u ra: káº¿t quáº£ Ä‘Ã¡nh giÃ¡ an toÃ n.\n",
        "\n",
        "> âš ï¸ Keep it simple and safe; do not expose full `eval` in real systems.\n",
        "> âš ï¸ Giá»¯ nÃ³ Ä‘Æ¡n giáº£n vÃ  an toÃ n; khÃ´ng Ä‘á»ƒ lá»™ toÃ n bá»™ `eval` trong há»‡ thá»‘ng thá»±c táº¿.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233008e9",
      "metadata": {
        "id": "233008e9"
      },
      "outputs": [],
      "source": [
        "def calculator_tool(expression: str) -> str:\n",
        "    # Evaluate a simple math expression like '2 + 3 * 4'.\n",
        "    # ÄÃ¡nh giÃ¡ má»™t biá»ƒu thá»©c toÃ¡n há»c Ä‘Æ¡n giáº£n nhÆ° '2 + 3 * 4'.\n",
        "    try:\n",
        "        # Chá»‰ cho phÃ©p sá»­ dá»¥ng cÃ¡c hÃ m tá»« module math Ä‘á»ƒ Ä‘áº£m báº£o an toÃ n\n",
        "        allowed_names = {k: v for k, v in math.__dict__.items() if not k.startswith('__')}\n",
        "        result = eval(expression, {'__builtins__': {}}, allowed_names)\n",
        "        return f'Result: {result}'\n",
        "    except Exception as e:\n",
        "        return f'Error evaluating expression: {e}'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90a1edb",
      "metadata": {
        "id": "b90a1edb"
      },
      "source": [
        "## 8. Router Node â€“ Decide RAG vs Tool\n",
        "## 8. NÃºt Router â€“ Quyáº¿t Ä‘á»‹nh RAG hay Tool\n",
        "\n",
        "We implement a simple heuristic router:\n",
        "ChÃºng ta triá»ƒn khai má»™t router heuristic Ä‘Æ¡n giáº£n:\n",
        "\n",
        "- If the query contains digits AND math symbols â†’ route to `tool`.\n",
        "- Náº¿u truy váº¥n chá»©a chá»¯ sá»‘ VÃ€ kÃ½ hiá»‡u toÃ¡n há»c â†’ Ä‘á»‹nh tuyáº¿n Ä‘áº¿n `tool`.\n",
        "- Otherwise â†’ route to `rag`.\n",
        "- NgÆ°á»£c láº¡i â†’ Ä‘á»‹nh tuyáº¿n Ä‘áº¿n `rag`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5784290",
      "metadata": {
        "id": "c5784290"
      },
      "outputs": [],
      "source": [
        "def router_node(state: AgentState) -> AgentState:\n",
        "    # NÃºt router quyáº¿t Ä‘á»‹nh Ä‘iá»u hÆ°á»›ng Ä‘áº¿n RAG hay Tool\n",
        "    query = state.user_input.strip()\n",
        "    # Kiá»ƒm tra náº¿u truy váº¥n chá»©a sá»‘ vÃ  kÃ½ hiá»‡u toÃ¡n há»c\n",
        "    if re.search(r'[0-9]', query) and re.search(r'[+\\-*/]', query):\n",
        "        state.route = 'tool'\n",
        "    else:\n",
        "        state.route = 'rag'\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9129e4",
      "metadata": {
        "id": "3f9129e4"
      },
      "source": [
        "## 9. RAG Node â€“ Handle Study Queries\n",
        "## 9. NÃºt RAG â€“ Xá»­ lÃ½ truy váº¥n há»c táº­p\n",
        "\n",
        "This node will:\n",
        "NÃºt nÃ y sáº½:\n",
        "\n",
        "1. Log the user query to memory.\n",
        "1. Ghi láº¡i truy váº¥n ngÆ°á»i dÃ¹ng vÃ o bá»™ nhá»›.\n",
        "2. Call `answer_with_rag`.\n",
        "2. Gá»i `answer_with_rag`.\n",
        "3. Log the answer.\n",
        "3. Ghi láº¡i cÃ¢u tráº£ lá»i.\n",
        "4. Append both to `state.messages`.\n",
        "4. ThÃªm cáº£ hai vÃ o `state.messages`.\n",
        "5. Print the answer.\n",
        "5. In cÃ¢u tráº£ lá»i.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878dcc78",
      "metadata": {
        "id": "878dcc78"
      },
      "outputs": [],
      "source": [
        "def rag_node(state: AgentState) -> AgentState:\n",
        "    # NÃºt RAG xá»­ lÃ½ cÃ¡c truy váº¥n há»c táº­p\n",
        "    query = state.user_input\n",
        "    remember_message('user', query)  # Ghi nhá»› cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng\n",
        "    answer = answer_with_rag(query)  # Tráº£ lá»i sá»­ dá»¥ng RAG\n",
        "    remember_message('assistant', answer)  # Ghi nhá»› cÃ¢u tráº£ lá»i\n",
        "    state.messages.append({'role': 'user', 'content': query})\n",
        "    state.messages.append({'role': 'assistant', 'content': answer})\n",
        "    print(f'Agent (RAG): {answer}')\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dcb57c0",
      "metadata": {
        "id": "4dcb57c0"
      },
      "source": [
        "## 10. Tool Node â€“ Handle Utility Queries\n",
        "## 10. NÃºt Tool â€“ Xá»­ lÃ½ truy váº¥n tiá»‡n Ã­ch\n",
        "\n",
        "This node will:\n",
        "NÃºt nÃ y sáº½:\n",
        "\n",
        "1. Log the user query.\n",
        "1. Ghi láº¡i truy váº¥n ngÆ°á»i dÃ¹ng.\n",
        "2. Call `calculator_tool`.\n",
        "2. Gá»i `calculator_tool`.\n",
        "3. Store result in `state.tool_result`.\n",
        "3. LÆ°u káº¿t quáº£ vÃ o `state.tool_result`.\n",
        "4. Log answer + print it.\n",
        "4. Ghi láº¡i cÃ¢u tráº£ lá»i + in nÃ³.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f294bf0",
      "metadata": {
        "id": "2f294bf0"
      },
      "outputs": [],
      "source": [
        "def tool_node(state: AgentState) -> AgentState:\n",
        "    # NÃºt Tool xá»­ lÃ½ cÃ¡c truy váº¥n tiá»‡n Ã­ch (vÃ­ dá»¥: tÃ­nh toÃ¡n)\n",
        "    query = state.user_input\n",
        "    remember_message('user', query)  # Ghi nhá»› cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng\n",
        "    tool_output = calculator_tool(query)  # Gá»i cÃ´ng cá»¥ mÃ¡y tÃ­nh\n",
        "    state.tool_result = tool_output  # LÆ°u káº¿t quáº£ vÃ o state\n",
        "    remember_message('assistant', tool_output)  # Ghi nhá»› káº¿t quáº£\n",
        "    state.messages.append({'role': 'user', 'content': query})\n",
        "    state.messages.append({'role': 'assistant', 'content': tool_output})\n",
        "    print(f'Agent (TOOL): {tool_output}')\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24aaecab",
      "metadata": {
        "id": "24aaecab"
      },
      "source": [
        "## 11. Build the LangGraph\n",
        "## 11. XÃ¢y dá»±ng LangGraph\n",
        "\n",
        "We wire the graph:\n",
        "ChÃºng ta káº¿t ná»‘i Ä‘á»“ thá»‹:\n",
        "\n",
        "- Entry: `router`\n",
        "- Äiá»ƒm vÃ o: `router`\n",
        "- Conditional edges: `router -> rag_node` or `router -> tool_node`\n",
        "- Cáº¡nh cÃ³ Ä‘iá»u kiá»‡n: `router -> rag_node` hoáº·c `router -> tool_node`\n",
        "- Both `rag_node` and `tool_node` go to `END`.\n",
        "- Cáº£ `rag_node` vÃ  `tool_node` Ä‘á»u Ä‘i Ä‘áº¿n `END`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58043878",
      "metadata": {
        "id": "58043878"
      },
      "outputs": [],
      "source": [
        "def router_edge(state: AgentState) -> str:\n",
        "    # HÃ m quyáº¿t Ä‘á»‹nh cáº¡nh Ä‘iá»u hÆ°á»›ng tá»« router\n",
        "    if state.route == 'rag':\n",
        "        return 'rag_node'\n",
        "    elif state.route == 'tool':\n",
        "        return 'tool_node'\n",
        "    return 'rag_node'  # Máº·c Ä‘á»‹nh lÃ  rag_node\n",
        "\n",
        "# XÃ¢y dá»±ng Ä‘á»“ thá»‹ LangGraph\n",
        "graph_builder = StateGraph(AgentState)\n",
        "graph_builder.add_node('router', router_node)  # ThÃªm nÃºt router\n",
        "graph_builder.add_node('rag_node', rag_node)  # ThÃªm nÃºt RAG\n",
        "graph_builder.add_node('tool_node', tool_node)  # ThÃªm nÃºt Tool\n",
        "graph_builder.set_entry_point('router')  # Äáº·t Ä‘iá»ƒm vÃ o lÃ  router\n",
        "graph_builder.add_conditional_edges('router', router_edge)  # ThÃªm cáº¡nh cÃ³ Ä‘iá»u kiá»‡n tá»« router\n",
        "graph_builder.add_edge('rag_node', END)  # RAG node káº¿t thÃºc\n",
        "graph_builder.add_edge('tool_node', END)  # Tool node káº¿t thÃºc\n",
        "\n",
        "app = graph_builder.compile()\n",
        "print('âœ… LangGraph graph compiled (once you define all components).')\n",
        "print('âœ… Äá»“ thá»‹ LangGraph Ä‘Ã£ Ä‘Æ°á»£c biÃªn dá»‹ch (sau khi báº¡n Ä‘á»‹nh nghÄ©a táº¥t cáº£ cÃ¡c thÃ nh pháº§n).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21ccc7ea",
      "metadata": {
        "id": "21ccc7ea"
      },
      "source": [
        "## 12. Chat Loop â€“ Test Your Agent\n",
        "## 12. VÃ²ng láº·p Chat â€“ Kiá»ƒm tra Agent cá»§a báº¡n\n",
        "\n",
        "Finally, we define a simple console loop to interact with the agent.\n",
        "Cuá»‘i cÃ¹ng, chÃºng ta Ä‘á»‹nh nghÄ©a má»™t vÃ²ng láº·p console Ä‘Æ¡n giáº£n Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i agent.\n",
        "Run this only **after** you have implemented all TODOs above.\n",
        "Cháº¡y Ä‘iá»u nÃ y chá»‰ **sau khi** báº¡n Ä‘Ã£ triá»ƒn khai táº¥t cáº£ cÃ¡c TODO á»Ÿ trÃªn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f9509a",
      "metadata": {
        "id": "a8f9509a"
      },
      "outputs": [],
      "source": [
        "def chat_loop():\n",
        "    # VÃ²ng láº·p chat Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i agent\n",
        "    print('ğŸ”¹ Personal Study Copilot â€“ type \"exit\" or \"quit\" to stop.')\n",
        "    print('ğŸ”¹ Personal Study Copilot â€“ gÃµ \"exit\" hoáº·c \"quit\" Ä‘á»ƒ dá»«ng.')\n",
        "    while True:\n",
        "        user_input = input('\\nYou: ').strip()\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print('ğŸ‘‹ Goodbye!')\n",
        "            print('ğŸ‘‹ Táº¡m biá»‡t!')\n",
        "            break\n",
        "        # Táº¡o state má»›i vÃ  gá»i app\n",
        "        state = AgentState(user_input=user_input, messages=[])\n",
        "        _ = app.invoke(state)\n",
        "\n",
        "# Uncomment after everything is implemented:\n",
        "# Bá» comment sau khi má»i thá»© Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai:\n",
        "# chat_loop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f09db4f",
      "metadata": {
        "id": "9f09db4f"
      },
      "source": [
        "## 13. What to Submit\n",
        "## 13. Nhá»¯ng gÃ¬ cáº§n ná»™p\n",
        "\n",
        "- This completed notebook (.ipynb).\n",
        "- Notebook Ä‘Ã£ hoÃ n thÃ nh nÃ y (.ipynb).\n",
        "- Screenshots or short video of:\n",
        "- áº¢nh chá»¥p mÃ n hÃ¬nh hoáº·c video ngáº¯n vá»:\n",
        "  - RAG answering study questions from your documents.\n",
        "  - RAG tráº£ lá»i cÃ¢u há»i há»c táº­p tá»« tÃ i liá»‡u cá»§a báº¡n.\n",
        "  - Tool answering a math query.\n",
        "  - Tool tráº£ lá»i má»™t truy váº¥n toÃ¡n há»c.\n",
        "- Short README / last markdown cell describing:\n",
        "- README ngáº¯n / cell markdown cuá»‘i cÃ¹ng mÃ´ táº£:\n",
        "  - Which openâ€‘source models you used.\n",
        "  - CÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ báº¡n Ä‘Ã£ sá»­ dá»¥ng.\n",
        "  - Any limitations / issues.\n",
        "  - Báº¥t ká»³ háº¡n cháº¿ / váº¥n Ä‘á» nÃ o.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
